# 时间线: 2022.07.21
确认步骤:
1 确定使用内容: 关键词 摘要 关键词填充词 题目, 对数据进行简单过滤--去除含空内容的数据, 并按出版时间排序
2 清洗: 对 摘要 题目 进行去停用词 词性还原,  对 关键词 进行去停用词 去命名实体 去科学常用词 词性还原
3 强化: 对 摘要 题目 中的关键词进行预处理, 防止其在分词过程中被破坏
3.1 提速: 目的: 因为关键词太多了, 如果对每个摘要都需要确定是否存在关键词, 并进行关键词预处理太慢了, 所以做一个映射;
将关键词集做如下映射: {关键词某个单词: 关键词集含有该单词的所有关键词索引[]}, 这样就可以多摘要分词, 查看摘要中含有哪些可能的关键词
大大降低摘要对关键词的预处理规模
4 对出版年份进行分段划分: 使用分段线性表示法, 将每个季度的关键词数作为y, 季度时间作为x进行分段划分
5 将关键词和摘要按照上面的分段进行分段聚合, 并将分段聚合的的摘要放到word2vec模型中训练, 以此获得该段时间的关键词词向量
并根据word2vec性质, 计算两两关键词的相似度, 超过阈值进行作边处理, 配合关键词作为节点, 做分段图网络
6 将分段图网络进行二点连通-louvain算法进行社区划分, 对划分的每个社区, 根据中心度最大前n个确定该社区的核心节点
对划分好的社区, 使用三边连通, 是相当于: 对某个时间段的研究方向, 进行进一步的筛选, 筛选出该方向的核心团体(相互间关联度更高)
它和使用Z-Score>2.5确定核心团体节点有一定的差别, 前者是团体中的核心(类似与毛主席&习近平), 我们筛选出的是(包含毛主席&习近平的紧密团体)
此时, 我们的希望是: 对这个方向进行计算, 它与下一个核心方向, 是否存在关联的分裂
7 对每个社区的关键词, 都在最大word2vec模型中计算, 计算相邻年段中, 具有超过阈值的相似度, 以此判断核心节点的演变

问题1: 时间划分 应该根据什么条件划分--季度中的关键词增加减少数量 & 总关键词的数量 & 转折点的辨认
问题2: 在摘要关键词预处理中, 是否将整个摘要当做一句话, 形成上下句关联性, 如果是, 则社区会较为紧密, 如果不是, 则社区会较为稀疏
问题3: 在摘要关键词预处理中, 关键词是否进行神奇处理: 添加摘要关键词进入关键词组(理论上如果加入, 则摘要关键词会有更大的权重), 
      或者将本身关键词进行虚化处理, 即根据巨大语料库网络的word2vec模型, 计算摘要中每个词和关键词的相似度, 将部分和关键词相似度高的摘要词放入本身关键词
问题4: 根据[确认步骤6], 是否应该加强社区方面的讨论, 如社区间的更替--例子: 文本挖掘相关内容在下一领域的更替, 更加强调社区的概念, 而不是单个关键词的概念
      important!

需求1: 电脑内存实在有点不太充裕了, 可能需要+2条16g的相同的内存条, 或者直接买4条32g的内存条


# word2vec参数解释
vector_size: 词向量的维数
window: 句子当前单词和预测单词的最大距离
min_count: 忽略总频率低于此的词
workers: 使用多个线程一起运行
sg: {0, 1}, 1训练skip-gram, 0训练CBOW
hs: {0, 1}, 1使用softmax训练, 0使用负采样训练
negative: 指定负采样的噪声数, 建议{5-20}
ns_exponent: 负采样分布指数, 论文建议0,75
cbow_mean: {0, 1}, 在训练CBOW时, 0上下文词向量之和, 1上下文词向量平均值
alpha: 初始学习速率
min_alpha: 随着时间, 学习速率将线性下降至此
seed: 随机数种子设定, 对于词向量初始化使用, 方便复现
max_final_vocab: 设置每个词的学习次数上限
sample: 设置高频词的降采样阈值, 有效范围 (0, 1e-5)
epochs: 语料库迭代次数


# 时间线: 现在2022.08.01
我们观察月份分区折线图, 可以发现一个现象: 大部分的异常凸点, 都是在每年的1月
对于-年份分区折线图-, 显著凸起点分别为: 
2010.01, 2011.01, 2012.01, 2013.01, 2014.01, 2015.01, 
2016.01, 2017.01, 2018.01, 2019.05, 2020.01, 2020.11, 
2021.01, 2021.05, 2022.01
# 摘要做成语料库, word2vec词向量, 
# 问题1: word2vec相似度太高
# 问题2: 3个词窗口共现网络 -- 如何定义相似度, node2vec(缺点: 很难训练, 训练结果不确定, 节点与节点距离, 不一定大的就大, 2度邻居)


# 时间线: 现在2022.08.10
我们需要对一月份的论文数据进行处理, 因为有一个日本期刊, 只有一月份出刊, 导致一月份关键词过多
不仅如此, 1月1刊, 2月1刊, 3月1刊, 纯纯恶心世界
现在这样处理, 根据DOI顺序, 对每个期刊都平均到月进行归一化


# 时间线: 现在2022.08.14
整个流程: 
1 过滤没有 关键词 & 摘要 & 标题 & 出版年月 的数据, 然后再将数据按照出版年月排序, 并且, 对于同一出版年月的数据, 按照DOI进行二次排序
  然后, 将每个期刊的数据, 进行按月平均, 如: SCIENTOMETRICS期刊缺失2018-01 2018-02的数据, 则利用2018-03的数据, 平均到这三个月
2 对每条数据的 关键词(关键词+摘要中与关键词相似度最高的3个词) 进行 去停用词 去科学常用词 词性还原 处理
  对 摘要(标题+摘要) 进行 去停用词 去前100高频词 词性还原 保留关键词(将关键词使用 _ 连接) 处理, 
  并使用 句子(用于word2vec) & 摘要(用于node2vec & jaccard) 两种模式对 摘要 进行存储
3 使用分段线性法对于以下折线图进行分段 -- x为出版年月, y为该出版年月的关键词种类数 -- , 确定时间分区
  分段线性法-自顶向下TD算法：https://www.cnblogs.com/by1990/archive/2011/01/15/1936296.html
  我基于此提了一点优化：限制每个时间分区最小必须跨越5个月份
4 将 关键词 & 摘要(两种模式) 按照时间分区聚集
5 对按时间分区聚集的 关键词 配合摘要 作 图网络 & 模型
  word2vec: 利用每个时间分区的 摘要(句子集合) 做训练集, 分别训练出每个时间分区的word2vec模型, 
            模型参数: 剔除词频<=3, 二次采样阈值=1e-4,  采样窗口=3, 负采样噪声k=5, 模型类型: skip gram跳字模型, 词向量维度=100
            参考链接: https://github.com/ShusenTang/Dive-into-DL-PyTorch/blob/master/docs/chapter10_natural-language-processing/10.3_word2vec-pytorch.md
            利用该word2vec模型, 计算时间分区内的关键词相似度, 将所有 相似度 > 阈值0.3 的两个关键词构建边, 制作出 关键词图网络, 
            同时, 利用相邻时间分区的 摘要(句子集合) 训练出融合年代的word2vec模型和关键词图网络
            利用所有 摘要(句子集合) 训练出一个全时区word2vec模型 和 全时区关键词图网络
  node2vec: 利用每个时间分区的每个 摘要, 设置共现窗口为3, 将所有摘要词作为点, 根据是否在共现窗口出现构造边, 制作出 摘要图
            然后从摘要图中, 制作出仅含以关键词为节点的子图.
               子图的边构造如下: 如果任意两个关键词在原图中是直连的, 则子图中这两个关键词也是直连的, 
            如果任意两个关键词在原图中不是直连的, 则判断这两个关键词的最短路中, 是否包含其他该子图的点, 如果包含, 则不处理, 如果不包含, 
            则对这两个关键词构建新边.
               然后将每个时间分区内的所有子图各自合并, 形成各自时间分区的 关键词图网络. 然后将各自的 关键词图网络 做训练集, 
            分别训练出各自时间分区的node2vec模型--node2vec模型是利用多次随机游走重新制作新句子集, 然后根据新句子集进行word2vec训练.
            同时, 利用相邻时间分区的 关键词图网络 训练出融合年代的node2vec模型和关键词图网络
            利用所有 关键词图网络 训练出一个全时区node2vec模型 和全时区关键词图网络 
            模型参数: 负采样词数量=30
            参考链接: https://www.bilibili.com/video/BV1BS4y1E7tf
  jaccard: 利用每个时间分区的每个 摘要, 设置共现窗口为3, 将所有摘要词作为点, 根据是否在共现窗口出现进行构造边, 制作出 摘要图
           然后从摘要图中, 制作出仅含以关键词为节点的子图.
               子图的边构造如下: 如果任意两个关键词在原图中是直连的, 则子图中这两个关键词也是直连的, 
           如果任意两个关键词在原图中不是直连的, 则判断这两个关键词的最短路中, 是否包含其他该子图的点, 如果包含, 则不处理, 如果不包含, 
           则对这两个关键词构建新边.
               然后将每个时间分区内的所有子图各自合并, 形成各自时间分区的 关键词图网络. 同时, 
           利用相邻时间分区的 关键词图网络 制作融合年代的关键词图网络, 利用所有 关键词图网络 制作全时区关键词图网络.
           并且, jaccard不制作模型.
6 对每个时间分区的 关键词图网络 进行二点连通-louvain划分, 得到每个时间分区的社区子图, 
    使用度中心性(代表社交关联强度)>0.1或z-得分(代表社交相似强度)>2.5筛选出主节点, 作为该子社区的关键词(社区代表词)
      定义一套新的相邻年代的演变方式: 对于相邻年代-(年代1, 年代2), 使用融合年代(两者图交集)来观察演变
      融合年代解释: 融合年代是两个图的并集, 它代表着新老社区的混合, 同时也隐含着新老社区的关联
      通过观察老年代和融合年代的社区划分, 可以知道老年代在演变过程中, 逐渐和那些词有了新的交集
      再通过观察新社区的社区划分, 了解到新年代的词又如何进行关联
  过程: 老年代社区划分, 融合年代社区划分, 新年代社区划分, 老年代和新年代进行社区代表词筛选, 融合年代不进行筛选
    计算老年代每个社区与融合年代的所有社区的交集点数量, 选取 最大数量的融合年代社区 作为老年代与融合年代的演变,
    对于新年代也进行类似操作, 选取 最大数据的融合年代社区 作为融合年代与新年代的演变.
    此时, 老年代的社区和新年代的社区, 通过融合年代进行关联
7 利用 融合年代模型, 计算相邻时间分区的 社区代表词 之间的相关性
  其中, word2vec & node2vec 是利用模型, 将词转化成向量, 计算两个向量之间的余弦相似度
    其实只要新老年代的社区通过一个融合年代进行关联, 就说明他们的词是相关的, 此时只需要计算: 对于新年代社区的词, 老年代社区哪个词贡献最大即可
  jaccard是利用 融合年代图网络, 计算两个词之间的jaccard值: 
      如果这两个词在图网络中相邻, 则jaccard值 = len(词1邻居∩词2邻居) / len(词1邻居∪词2邻居)
      如果这两个词在图网络中最短路距离为2, 则jaccard值= 0.5 * len(词1邻居∩词2邻居) / len(词1邻居∪词2邻居)
      如果这两个词在图网络中最短路距离>2, 则jaccard值= 0
      我们依旧选取 值最大 作为新老词的关联
8 利用6的演变, 制作时间社区演绎图: 该图只包含有边相连的社区点, 主要用来发现 社区的演绎
9 利用6的演变, 制作时间社区全状态演绎图: 该图包含所有的 社区 , 同时根据颜色, 显示出社区演绎的六种状态
  新生: 红色   消亡: 蓝色   继承: 绿色   分裂: 紫色   融合: 黄色   孤立: 棕色   其他: 灰色   融合中介社区: 粉色
10 统计了每个时区的关键词词频情况, 可以直观感受关键词的演绎, 主要利用该图评价我们算法的演绎情况

# 社区演绎
新生: T时间窗口未出现, T+1时间窗口出现, 与T时间窗内主题无关联, 且与T+2时间窗内主题有关联的主题
消亡: T时间窗出现, T+1时间窗没有出现, 且与T+1时间窗内主题无关联的主题
继承: T时间窗内主题i和T+1时间窗内主题j, 两者间有且只有一条关联, 则二者为继承关系
分裂: T时间窗内主题i和T+1时间窗内多个主题存在较强关联, 则认为主题i发生分裂
融合: T+1时间窗内主题j与T时间窗内多个主题存在较强关联, 则认为主题j是融合发展的主题
孤立: T+1时间窗内主题j与T时间窗内所有主题以及T+2时间窗内所有主题均无关联, 只孤立的出现在当前时间窗


# 时间线: 现在2022.08.17
词1-词2相似度 0.9成为核心节点 可能性 > 词3-词5相似度0.4, 词3-词4相似度0.3 == 0.7

z-得分: 词1和词2是主节点:  
度中心性: 词3是主节点: 

防止杂点对主社区的影响: 使用度中心性(代表社交关联强度)或z-得分(代表社交相似强度)筛选出主节点
然后定义一套新的相邻年代的演变方式: 对于相邻年代-(年代1, 年代2), 使用融合年代(两者图交集)来重新定义演变
解释融合年代: 融合年代是两个图的并集, 它代表着新老社区的混合, 同时也隐含着新老社区的关联
通过发现老社区和融合社区的演变, 可以知道老社区在演变过程中, 逐渐和那些词有了新的交集, 哪些词又在后续中关联度降低
再通过融合社区的新社区的演变, 形成: 老社区-->融合社区-->新社区的过度情况
通过发现老社区 新社区与融合社区的关联度, 间接的关联了新老社区的演变过程
再通过融合社区, 计算新老社区的词的相关性, 进一步发现是那些词发生了演变


1 流程图
2 分段线性 解释图
3 二点连通-louvain图(中文词删除)
4 宏观社区演化图, 放大一个演示, 放大社区 || 社区点之间的线重画一个新图
5 度中心性等指标相关图 + z-得分
6 6个演变的图

# 2022.08.22
图大部分做完, 代码重构完, 代表着新的开始